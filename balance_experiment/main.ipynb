{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['__NV_PRIME_RENDER_OFFLOAD'] = '1'\n",
    "os.environ['__GLX_VENDOR_LIBRARY_NAME'] = 'nvidia'\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "\n",
    "from functools import partial\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "import wandb\n",
    "\n",
    "\n",
    "import jax\n",
    "import mediapy as media\n",
    "from randomize import domain_randomize\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)\n",
    "\n",
    "import balance\n",
    "env = balance.G1Env()\n",
    "eval_env = balance.G1Env()\n",
    "env_cfg = balance.default_config()\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "env_name = \"g1_balance\"\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "exp_name = f\"{env_name}-{timestamp}\"\n",
    "import os\n",
    "\n",
    "ckpt_path = os.path.abspath(os.path.join(\".\", \"checkpoints\", exp_name))\n",
    "os.makedirs(ckpt_path, exist_ok=True)\n",
    "print(f\"Checkpoint path: {ckpt_path}\")# media.show_video(frames, fps=1.0 / env.dt)\n",
    "\n",
    "\n",
    "wandb.init(project=\"mjxrl\", config=env_cfg)\n",
    "wandb.config.update({\n",
    "    \"env_name\": env_name,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)\n",
    "# state = jit_reset(jax.random.PRNGKey(0))\n",
    "# rollout = [state]\n",
    "# f = 0.5\n",
    "# from tqdm import tqdm\n",
    "# import jax.numpy as jp\n",
    "# for i in tqdm(range(200)):\n",
    "#   action = []\n",
    "#   for j in range(env.action_size):\n",
    "\n",
    "#     if env.mj_model.actuator(j).name == \"right_knee_joint\" or env.mj_model.actuator(j).name == \"left_shoulder_roll_joint\":\n",
    "#       value = jp.sin(\n",
    "#             state.data.time * 2 * jp.pi * f \n",
    "#         ) * 1.\n",
    "#     else:\n",
    "#       value = 0.\n",
    "      \n",
    "#     action.append( value)\n",
    "#   action = jp.array(action)\n",
    "#   state = jit_step(state, action)\n",
    "#   rollout.append(state)\n",
    "# frames = env.render(rollout,camera=\"track\")\n",
    "# frames_np = np.array(frames)\n",
    "# frames_np_rearranged = np.transpose(frames_np, (0, 3, 1, 2))\n",
    "# wandb.log({\"video\": wandb.Video(frames_np_rearranged, fps=1.0 / env.dt, format=\"gif\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_collections import config_dict\n",
    "# ppo_params= config_dict.create(\n",
    "#     num_timesteps=1_000_000,\n",
    "#     num_evals=10,\n",
    "#     reward_scaling=10.0,\n",
    "#     episode_length=env_cfg.episode_length,\n",
    "#     normalize_observations=True,\n",
    "#     action_repeat=1,\n",
    "#     unroll_length=30,\n",
    "#     num_minibatches=32,\n",
    "#     num_updates_per_batch=16,\n",
    "#     discounting=0.995,\n",
    "#     learning_rate=1e-3,\n",
    "#     entropy_cost=1e-2,\n",
    "#     num_envs=2048,\n",
    "#     batch_size=1024,\n",
    "# )\n",
    "# from ml_collections import config_dict\n",
    "# ppo_params= config_dict.create(\n",
    "#     num_timesteps=100_000_000,\n",
    "#     reward_scaling=10.0,\n",
    "#     episode_length=env_cfg.episode_length,\n",
    "#     normalize_observations=True,\n",
    "#     action_repeat=1,\n",
    "#     unroll_length=30,\n",
    "#     num_minibatches=32,\n",
    "#     num_updates_per_batch=16,\n",
    "#     discounting=0.995,\n",
    "#     learning_rate=1e-3,\n",
    "#     entropy_cost=1e-2,\n",
    "#     num_envs=2048,\n",
    "#     batch_size=1024,\n",
    "#     num_evals=0,\n",
    "#     log_training_metrics=True,\n",
    "#     network_factory=config_dict.create(\n",
    "#         policy_hidden_layer_sizes=(512, 256, 128),\n",
    "#         value_hidden_layer_sizes=(512, 256, 128),\n",
    "#         policy_obs_key=\"state\",\n",
    "#         value_obs_key=\"privileged_state\",\n",
    "#     )\n",
    "# )\n",
    "from ml_collections import config_dict\n",
    "ppo_params= config_dict.create(\n",
    "    num_timesteps=400_000_000,\n",
    "    reward_scaling=0.1,\n",
    "    episode_length=env_cfg.episode_length,\n",
    "    normalize_observations=True,\n",
    "    action_repeat=1,\n",
    "    unroll_length=32,\n",
    "    num_minibatches=32,\n",
    "    num_updates_per_batch=5,\n",
    "    discounting=0.98,\n",
    "    learning_rate=1e-4,\n",
    "    entropy_cost=0,\n",
    "    num_envs=32768,\n",
    "    batch_size=2048,\n",
    "    num_evals=16,\n",
    "    log_training_metrics=True,\n",
    "    network_factory=config_dict.create(\n",
    "        policy_hidden_layer_sizes=(512, 256, 64),\n",
    "        value_hidden_layer_sizes=(256, 256, 256,256),\n",
    "        # policy_obs_key=\"state\",\n",
    "        value_obs_key=\"privileged_state\"\n",
    "    )\n",
    ")\n",
    "x_data, y_data, y_dataerr = [], [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "\n",
    "def progress_cli(num_steps, metrics):\n",
    "  \"\"\"Prints progress metrics to the console, including all available metrics.\"\"\"\n",
    "\n",
    "  wandb.log(metrics, step=num_steps)\n",
    "\n",
    "  # Print the current step number\n",
    "  print(f\"Step: {num_steps}\")\n",
    "\n",
    "  # Print the entire metrics dictionary for debugging\n",
    "  print(\"Metrics:\", metrics)\n",
    "\n",
    "  # You can add a separator for clarity if you run this multiple times\n",
    "  print(\"-\" * 20)\n",
    "  # # Assuming `jit_reset` and `jit_step` are already JIT-compiled\n",
    "\n",
    "\n",
    "\n",
    "def policy_params_fn(current_step, make_policy, params):\n",
    "  del make_policy\n",
    "  print(\"Policy params fn\")\n",
    "  jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))\n",
    "  rng = jax.random.PRNGKey(42)\n",
    "  state = jit_reset(rng)  # Initialize the environment state\n",
    "  rollout = []\n",
    "\n",
    "  # Run the rollout until termination\n",
    "  while not state.done:  # Assuming `state.done` indicates termination\n",
    "      rng, act_rng = jax.random.split(rng)\n",
    "      ctrl, _ = jit_inference_fn(state.obs, act_rng)  # Get action from the policy\n",
    "      state = jit_step(state, ctrl)  # Step the environment\n",
    "      rollout.append(state)\n",
    "\n",
    "  # Render and log the video\n",
    "  frames = eval_env.render(rollout, camera=\"track\")\n",
    "  frames_np = np.array(frames)\n",
    "  frames_np_rearranged = np.transpose(frames_np, (0, 3, 1, 2))\n",
    "  wandb.log({\"video\": wandb.Video(frames_np_rearranged, fps=1.0 / env.dt, format=\"gif\")}, step=current_step)\n",
    "\n",
    "\n",
    "ppo_training_params = dict(ppo_params)\n",
    "network_factory = ppo_networks.make_ppo_networks\n",
    "if \"network_factory\" in ppo_params:\n",
    "  del ppo_training_params[\"network_factory\"]\n",
    "  network_factory = partial(\n",
    "      ppo_networks.make_ppo_networks,\n",
    "      **ppo_params.network_factory\n",
    "  )\n",
    "from mujoco_playground import wrapper\n",
    "\n",
    "train_fn = partial(\n",
    "    ppo.train, **dict(ppo_training_params),\n",
    "    network_factory=network_factory,\n",
    "    progress_fn=progress_cli,\n",
    "    # policy_params_fn=policy_params_fn,\n",
    "    randomization_fn=domain_randomize,\n",
    "    wrap_env_fn=wrapper.wrap_for_brax_training,\n",
    "    save_checkpoint_path=ckpt_path  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_inference_fn, params, metrics = train_fn(\n",
    "    environment=env,\n",
    "    eval_env=eval_env\n",
    ")\n",
    "# print(f\"time to jit: {times[1] - times[0]}\")\n",
    "# print(f\"time to train: {times[-1] - times[1]}\")\n",
    "# # ~6m11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rollout = []\n",
    "n_episodes = 1\n",
    "\n",
    "for _ in range(n_episodes):\n",
    "  state = jit_reset(rng)\n",
    "  rollout.append(state)\n",
    "  for i in range(env_cfg.episode_length):\n",
    "    act_rng, rng = jax.random.split(rng)\n",
    "    ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "    state = jit_step(state, ctrl)\n",
    "    rollout.append(state)\n",
    "\n",
    "\n",
    "frames = eval_env.render(rollout, camera=\"track\")\n",
    "frames_np = np.array(frames)\n",
    "frames_np_rearranged = np.transpose(frames_np, (0, 3, 1, 2))\n",
    "wandb.log({\"video\": wandb.Video(frames_np_rearranged, fps=1.0 / env.dt, format=\"gif\")})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media.show_video(frames, fps=1.0 / env.dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
