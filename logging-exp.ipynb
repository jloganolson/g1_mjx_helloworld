{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['__NV_PRIME_RENDER_OFFLOAD'] = '1'\n",
    "os.environ['__GLX_VENDOR_LIBRARY_NAME'] = 'nvidia'\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "\n",
    "# import mujoco\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import checkpoint as ppo_checkpoint\n",
    "\n",
    "from IPython.display import  clear_output\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapy as media\n",
    "from tqdm import tqdm\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import g1hello\n",
    "env = partial(g1hello.G1_23dof)()\n",
    "env_cfg = g1hello.default_config()\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# state = jit_reset(jax.random.PRNGKey(0))\n",
    "# rollout = [state]\n",
    "\n",
    "# f = 0.5\n",
    "\n",
    "# for i in tqdm(range(200)):\n",
    "#   action = []\n",
    "#   for j in range(env.action_size):\n",
    "\n",
    "#     if env.mj_model.actuator(j).name == \"right_knee_joint\" or env.mj_model.actuator(j).name == \"left_shoulder_roll_joint\":\n",
    "#       value = jp.sin(\n",
    "#             state.data.time * 2 * jp.pi * f \n",
    "#         ) * 1.\n",
    "#     else:\n",
    "#       value = 0.\n",
    "      \n",
    "#     action.append( value)\n",
    "#   action = jp.array(action)\n",
    "#   state = jit_step(state, action)\n",
    "#   rollout.append(state)\n",
    "# frames = env.render(rollout)\n",
    "# media.show_video(frames, fps=1.0 / env.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "env_name = \"arm_hello_world\"\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "exp_name = f\"{env_name}-{timestamp}\"\n",
    "import os\n",
    "\n",
    "ckpt_path = os.path.abspath(os.path.join(\".\", \"checkpoints\", exp_name))\n",
    "os.makedirs(ckpt_path, exist_ok=True)\n",
    "print(f\"Checkpoint path: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_collections import config_dict\n",
    "ppo_params= config_dict.create(\n",
    "    num_timesteps=1_000_000,\n",
    "    reward_scaling=10.0,\n",
    "    episode_length=env_cfg.episode_length,\n",
    "    normalize_observations=True,\n",
    "    action_repeat=1,\n",
    "    unroll_length=30,\n",
    "    num_minibatches=32,\n",
    "    num_updates_per_batch=16,\n",
    "    discounting=0.995,\n",
    "    learning_rate=1e-3,\n",
    "    entropy_cost=1e-2,\n",
    "    num_envs=512, #made these smaller just to see some logging feedback\n",
    "    batch_size=256,#made these smaller just to see some logging feedback\n",
    "    num_evals=0\n",
    ")\n",
    "\n",
    "x_data, y_data, y_dataerr = [], [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "  clear_output(wait=True)\n",
    "\n",
    "  times.append(datetime.now())\n",
    "  x_data.append(num_steps)\n",
    "  y_data.append(metrics[\"eval/episode_reward\"])\n",
    "  y_dataerr.append(metrics[\"eval/episode_reward_std\"])\n",
    "\n",
    "  plt.xlim([0, ppo_params[\"num_timesteps\"] * 1.25])\n",
    "  plt.ylim([0, 1100])\n",
    "  plt.xlabel(\"# environment steps\")\n",
    "  plt.ylabel(\"reward per episode\")\n",
    "  plt.title(f\"y={y_data[-1]:.3f}\")\n",
    "  plt.errorbar(x_data, y_data, yerr=y_dataerr, color=\"blue\")\n",
    "\n",
    "  display(plt.gcf())\n",
    "\n",
    "def progress_cli(num_steps, metrics):\n",
    "  \"\"\"Prints progress metrics to the console, including all available metrics.\"\"\"\n",
    "\n",
    "  # Print the current step number\n",
    "  print(f\"Step: {num_steps}\")\n",
    "\n",
    "  # Print the entire metrics dictionary for debugging\n",
    "  print(\"Metrics:\", metrics)\n",
    "\n",
    "  # You can add a separator for clarity if you run this multiple times\n",
    "  print(\"-\" * 20)\n",
    "\n",
    "ppo_training_params = dict(ppo_params)\n",
    "network_factory = ppo_networks.make_ppo_networks\n",
    "if \"network_factory\" in ppo_params:\n",
    "  del ppo_training_params[\"network_factory\"]\n",
    "  network_factory = partial(\n",
    "      ppo_networks.make_ppo_networks,\n",
    "      **ppo_params.network_factory\n",
    "  )\n",
    "\n",
    "train_fn = partial(\n",
    "    ppo.train, **dict(ppo_training_params),\n",
    "    network_factory=network_factory,\n",
    "    progress_fn=progress_cli,\n",
    "    # policy_params_fn=policy_params_fn,\n",
    "    log_training_metrics=True,\n",
    "    save_checkpoint_path=ckpt_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mujoco_playground import wrapper\n",
    "\n",
    "make_inference_fn, params, metrics = train_fn(\n",
    "    environment=env,\n",
    "    wrap_env_fn=wrapper.wrap_for_brax_training,\n",
    ")\n",
    "\n",
    "# ~6m11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rollout = []\n",
    "n_episodes = 1\n",
    "\n",
    "for _ in range(n_episodes):\n",
    "  state = jit_reset(rng)\n",
    "  rollout.append(state)\n",
    "  for i in range(env_cfg.episode_length):\n",
    "    act_rng, rng = jax.random.split(rng)\n",
    "    ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "    state = jit_step(state, ctrl)\n",
    "    rollout.append(state)\n",
    "\n",
    "render_every = 1\n",
    "frames = env.render(rollout[::render_every])\n",
    "rewards = [s.reward for s in rollout]\n",
    "media.show_video(frames, fps=1.0 / env.dt / render_every)\n",
    "# ~11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load policy from scratch\n",
    "import g1hello\n",
    "from functools import partial\n",
    "import mediapy as media\n",
    "from brax.training.agents.ppo import checkpoint as ppo_checkpoint\n",
    "import jax\n",
    "from pathlib import Path\n",
    "\n",
    "relative_path = Path(\"./checkpoints/arm_hello_world-20250409-202838/000001228800\")\n",
    "policy_fn = ppo_checkpoint.load_policy(relative_path.resolve())\n",
    "\n",
    "env = partial(g1hello.G1_23dof)()\n",
    "env_cfg = g1hello.default_config()\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "jit_inference_fn = jax.jit(policy_fn)\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rollout = []\n",
    "n_episodes = 1\n",
    "\n",
    "for _ in range(n_episodes):\n",
    "  state = jit_reset(rng)\n",
    "  rollout.append(state)\n",
    "  for i in range(200):\n",
    "    act_rng, rng = jax.random.split(rng)\n",
    "    ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "    state = jit_step(state, ctrl)\n",
    "    rollout.append(state)\n",
    "\n",
    "render_every = 1\n",
    "frames = env.render(rollout[::render_every])\n",
    "rewards = [s.reward for s in rollout]\n",
    "\n",
    "\n",
    "media.show_video(frames, fps=1.0 / env.dt / render_every)\n",
    "# ~11s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
